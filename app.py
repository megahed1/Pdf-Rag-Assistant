import streamlit as st
import pdfplumber
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from groq import Groq

# --------------------------
# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù€ API
# --------------------------
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except FileNotFoundError:
    st.error("Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø±Ø§Ø± secrets.toml Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯! ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¦Ù‡ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ .streamlit")
    st.stop()
except KeyError:
    st.error("Ø§Ù„Ù…ÙØªØ§Ø­ GROQ_API_KEY Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ù…Ù„Ù secrets.toml")
    st.stop()
client = Groq(api_key=GROQ_API_KEY)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")


# --------------------------
# 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† PDF
# --------------------------
def extract_text_from_pdf(pdf_file):
    text = ""
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text


# --------------------------
# 3. Ø¥Ù†Ø´Ø§Ø¡ Embeddings
# --------------------------
def create_embeddings(chunks):
    vectors = embed_model.encode(chunks)
    vectors = np.array(vectors).astype("float32")
    return vectors


# --------------------------
# 4. Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª FAISS
# --------------------------
def build_faiss_index(vectors):
    dimension = vectors.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(vectors)
    return index


# --------------------------
# 5. RAG â€” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£Ù‚Ø±Ø¨ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ
# --------------------------
def search(query, chunks, index, k=3):
    query_vec = embed_model.encode([query]).astype("float32")
    distances, indices = index.search(query_vec, k)
    retrieved = "\n".join([chunks[i] for i in indices[0]])
    return retrieved


# --------------------------
# 6. Groq LLM â€” Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
# --------------------------
def ask_groq(question, context):
    prompt = f"""
You are a helpful AI assistant. Answer the question based on the context.

Context:
{context}

Question: {question}

Answer:
    """
    chat_completion = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}],
    )
    return chat_completion.choices[0].message.content


# --------------------------
# 7. ÙˆØ§Ø¬Ù‡Ø© Streamlit
# --------------------------
st.title("ğŸ“„ PDF RAG Assistant with Groq")
st.write("Ø§Ø±ÙØ¹ PDF ÙˆØ§Ø³Ø£Ù„ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø¹Ù†Ù‡!")

pdf_file = st.file_uploader("Ø§Ø±ÙØ¹ PDF", type=["pdf"])

if pdf_file:
    st.success("ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ âœ”")

    st.write("ğŸ“Œ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ...")
    text = extract_text_from_pdf(pdf_file)

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
    chunks = text.split("\n")
    chunks = [c.strip() for c in chunks if len(c.strip()) > 10]

    st.write("ğŸ“Œ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Embeddings...")
    vectors = create_embeddings(chunks)

    st.write("ğŸ“Œ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
    index = build_faiss_index(vectors)

    question = st.text_input("â“ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ù„Ù:")

    if st.button("Ø¥Ø±Ø³Ø§Ù„"):
        with st.spinner("Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
            context = search(question, chunks, index)
            answer = ask_groq(question, context)

        st.subheader("ğŸ§  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        st.write(answer)
